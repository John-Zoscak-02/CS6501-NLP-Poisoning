{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601332c8",
   "metadata": {},
   "source": [
    "# Poisoning Tehniques for LLMS, A Llama2 study\n",
    "\n",
    "<img src=\"img/meta.png\"/>\n",
    "\n",
    "---\n",
    "\n",
    "**John Zoscak (jmz9sad@virginia.edu), Arthur Redfern (jsf7un@virginia.edu)**\n",
    "Department of Computer Science\\\n",
    "University of Virginia\\\n",
    "Charlottesville, VA 22903, USA\n",
    "\n",
    "***Abstract:*** Many people have heard of \"Nightshade\", a new text-to-image poisoning software which uses undetectable altercations to pixels in image data during fine-tuning of image-generative models to corrupt image generation. One slightly less complete, yet adjacent field of study is the poisoning of LLMs. There have been a number of poisoning techniques employed on LLMs (especially instruction tuned LLMs), including label poisoning in fine-tuning stages, as well as training data poisoning. In our research, we intend to analyze the effect that these existing methods have on Llama2, as well as investigate the possibility of creating alternative forms poisoning data. We intended to discover if context-based attacks can be used to compromise general task functionality and deeper functionalities of LLMs with similarly minimal samples as previous work. This project is a hands-on style NLP project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9fd8c9",
   "metadata": {},
   "source": [
    "## Quick Start\n",
    "\n",
    "You can follow the steps below to quickly get up and running with Llama 2 models. These steps will let you run quick inference locally. For more examples, see the [Llama 2 recipes repository](https://github.com/facebookresearch/llama-recipes). \n",
    "\n",
    "1. In a conda env with PyTorch / CUDA available clone and download this repository.\n",
    "\n",
    "2. In the top-level directory run:\n",
    "    ```bash\n",
    "    pip install -e .\n",
    "    ```\n",
    "3. Visit the [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and register to download the model/s.\n",
    "\n",
    "4. Once registered, you will get an email with a URL to download the models. You will need this URL when you run the download.sh script.\n",
    "\n",
    "5. Once you get the email, navigate to your downloaded llama repository and run the download.sh script. \n",
    "    - Make sure to grant execution permissions to the download.sh script\n",
    "    - During this process, you will be prompted to enter the URL from the email. \n",
    "    - Do not use the “Copy Link” option but rather make sure to manually copy the link from the email.\n",
    "\n",
    "6. Once the model/s you want have been downloaded, you can run the model locally using the command below:\n",
    "```bash\n",
    "torchrun --nproc_per_node 1 example_chat_completion.py \\\n",
    "    --ckpt_dir llama-2-7b-chat/ \\\n",
    "    --tokenizer_path tokenizer.model \\\n",
    "    --max_seq_len 512 --max_batch_size 6\n",
    "```\n",
    "**Note**\n",
    "- Replace  `llama-2-7b-chat/` with the path to your checkpoint directory and `tokenizer.model` with the path to your tokenizer model.\n",
    "- The `–nproc_per_node` should be set to the [MP](#inference) value for the model you are using.\n",
    "- Adjust the `max_seq_len` and `max_batch_size` parameters as needed.\n",
    "- This example runs the [example_chat_completion.py](example_chat_completion.py) found in this repository but you can change that to a different .py file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135890e7",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Different models require different model-parallel (MP) values:\n",
    "\n",
    "|  Model | MP |\n",
    "|--------|----|\n",
    "| 7B     | 1  |\n",
    "| 13B    | 2  |\n",
    "| 70B    | 8  |\n",
    "\n",
    "All models support sequence length up to 4096 tokens, but we pre-allocate the cache according to `max_seq_len` and `max_batch_size` values. So set those according to your hardware.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5ec70",
   "metadata": {},
   "source": [
    "### Fine-tuned Chat Models\n",
    "\n",
    "The fine-tuned models were trained for dialogue applications. To get the expected features and performance for them, a specific formatting defined in [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212)\n",
    "needs to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces).\n",
    "\n",
    "You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for [an example](https://github.com/facebookresearch/llama-recipes/blob/main/examples/inference.py) of how to add a safety checker to the inputs and outputs of your inference code.\n",
    "\n",
    "Examples using llama-2-7b-chat:\n",
    "\n",
    "```bash\n",
    "torchrun --nproc_per_node 1 example_chat_completion.py \\\n",
    "    --ckpt_dir llama-2-7b-chat/ \\\n",
    "    --tokenizer_path tokenizer.model \\\n",
    "    --max_seq_len 512 --max_batch_size 6\n",
    "```\n",
    "\n",
    "Llama 2 is a new technology that carries potential risks with use. Testing conducted to date has not — and could not — cover all scenarios.\n",
    "In order to help developers address these risks, we have created the [Responsible Use Guide](Responsible-Use-Guide.pdf). More details can be found in our research paper as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conda environment for autotraining llama2 models... \n",
    "#\n",
    "# conda create -n autotrain\n",
    "# conda activate autotrain\n",
    "# pip install autotrain-advanced\n",
    "# conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "# conda install -c \"nvidia/label/cuda-12.1.0\" cuda-nvcc\n",
    "#\n",
    "# The below command can be ran in the conda autotrain kernel...\n",
    "# autotrain llm is a python package, it is a collection of common libraries into a well forumlated collection of resources necessary for automating the finetuning of language models...\n",
    "# This below command needs to be modified for the purpose of improving\n",
    " \n",
    "# !autotrain llm \\\n",
    "# --train \\\n",
    "# --model ${MODEL_NAME} \\\n",
    "# --project-name ${PROJECT_NAME} \\\n",
    "# --data-path data/ \\\n",
    "# --text-column text \\\n",
    "# --lr ${LEARNING_RATE} \\\n",
    "# --batch-size ${BATCH_SIZE} \\\n",
    "# --epochs ${NUM_EPOCHS} \\\n",
    "# --block-size ${BLOCK_SIZE} \\\n",
    "# --warmup-ratio ${WARMUP_RATIO} \\\n",
    "# --lora-r ${LORA_R} \\\n",
    "# --lora-alpha ${LORA_ALPHA} \\\n",
    "# --lora-dropout ${LORA_DROPOUT} \\\n",
    "# --weight-decay ${WEIGHT_DECAY} \\\n",
    "# --gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "# --quantization ${QUANTIZATION} \\\n",
    "# --mixed-precision ${MIXED_PRECISION} \\\n",
    "# $( [[ \"$PEFT\" == \"True\" ]] && echo \"--peft\" ) \\\n",
    "# $( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0492e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autotrain",
   "language": "python",
   "name": "autotrain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
